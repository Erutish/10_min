Multilingual Bangla RAG System
A robust Retrieval-Augmented Generation (RAG) pipeline for answering Bangla and English questions over the "HSC26 Bangla 1st Paper" textbook using advanced OCR, text extraction, embedding, vector search, and LLM-based answer generation.

Setup Guide
Colab/Jupyter Environment:
Clone the repo or upload scripts to your workspace.


Prepare Google Drive for input/output (PDF, intermediate files, etc.).


Install dependencies:

 python
CopyEdit
!pip install -U pinecone sentence-transformers openai easyocr pdf2image pymupdf opencv-python pillow


For OCR model weights:

 python
CopyEdit
!mkdir -p /root/.EasyOCR/model
!wget -q -O /root/.EasyOCR/model/craft_mlt_25k.pth "https://huggingface.co/xiaoyao9184/easyocr/resolve/master/craft_mlt_25k.pth"
!wget -q -O /root/.EasyOCR/model/bengali.pth "https://huggingface.co/xiaoyao9184/easyocr/resolve/master/bengali.pth"


Set your Pinecone and OpenAI API keys in the scripts.


Upload the Bangla PDF (e.g., Book.pdf) to Drive.


 Used Tools, Libraries, and Packages
OCR & Text Extraction:


easyocr (for Bangla OCR, robust on scans)


pdf2image, pymupdf (for direct PDF text extraction)


opencv-python, pillow (image pre-processing)


LLM Integration:


openai (for GPT-4.1-mini or GPT-4 generation)


Embedding & Retrieval:


sentence-transformers (intfloat/multilingual-e5-large)


pinecone (vector database)


Evaluation:


pandas (analysis, CSVs)


sentence-transformers (cosine similarity scoring)



üìö Sample Queries and Outputs
User Query (Bangla)
Model Answer
Ground Truth
Cosine Similarity
‚Äò‡¶Ö‡¶™‡¶∞‡¶ø‡¶ö‡¶ø‡¶§‡¶æ' ‡¶ó‡¶≤‡ßç‡¶™‡¶ü‡¶ø ‡¶™‡ßç‡¶∞‡¶•‡¶Æ ‡¶ï‡ßã‡¶•‡¶æ‡¶Ø‡¶º ‡¶™‡ßç‡¶∞‡¶ï‡¶æ‡¶∂‡¶ø‡¶§ ‡¶π‡¶Ø‡¶º?
‚Äò‡¶Ö‡¶™‡¶∞‡¶ø‡¶ö‡¶ø‡¶§‡¶æ‚Äô ‡¶ó‡¶≤‡ßç‡¶™‡¶ü‡¶ø ‡¶™‡ßç‡¶∞‡¶•‡¶Æ ‡¶™‡ßç‡¶∞‡¶ï‡¶æ‡¶∂‡¶ø‡¶§ ‡¶π‡¶Ø‡¶º ‡¶™‡ßç‡¶∞‡¶Æ‡¶• ‡¶ö‡ßå‡¶ß‡ßÅ‡¶∞‡ßÄ ‡¶∏‡¶Æ‡ßç‡¶™‡¶æ‡¶¶‡¶ø‡¶§ ‡¶Æ‡¶æ‡¶∏‡¶ø‡¶ï ‚Äò‡¶∏‡¶¨‡ßÅ‡¶ú‡¶™‡¶§‡ßç‡¶∞‚Äô ‡¶™‡¶§‡ßç‡¶∞‡¶ø‡¶ï‡¶æ‡¶∞ ‡ßß‡ß©‡ß®‡ßß ‡¶¨‡¶ô‡ßç‡¶ó‡¶æ‡¶¨‡ßç‡¶¶‡ßá‡¶∞ ‡¶ï‡¶æ‡¶∞‡ßç‡¶§‡¶ø‡¶ï ‡¶∏‡¶Ç‡¶ñ‡ßç‡¶Ø‡¶æ‡¶Ø‡¶º‡•§
‡¶∏‡¶¨‡ßÅ‡¶ú‡¶™‡¶§‡ßç‡¶∞
0.760399580001831
‡¶Ö‡¶®‡ßÅ‡¶™‡¶Æ‡ßá‡¶∞ ‡¶™‡ßç‡¶∞‡¶ï‡ßÉ‡¶§ ‡¶Ö‡¶≠‡¶ø‡¶≠‡¶æ‡¶¨‡¶ï ‡¶ï‡ßá?
‡¶Ö‡¶®‡ßÅ‡¶™‡¶Æ‡ßá‡¶∞ ‡¶™‡ßç‡¶∞‡¶ï‡ßÉ‡¶§ ‡¶Ö‡¶≠‡¶ø‡¶≠‡¶æ‡¶¨‡¶ï ‡¶§‡¶æ‡¶∞ ‡¶Æ‡¶æ‡¶Æ‡¶æ‡•§
‡¶Æ‡¶æ‡¶Æ‡¶æ‡¶ï‡ßá
0.818320751190185
 How was Horish define in book?
‡¶∞‡¶∏‡¶ø‡¶ï ‡¶Æ‡¶®‡ßá‡¶∞ ‡¶Æ‡¶æ‡¶®‡ßÅ‡¶∑
‡¶π‡¶∞‡¶ø‡¶∂‡¶ï‡ßá ‡¶¨‡¶á‡¶Ø‡¶º‡ßá ‡¶è‡¶Æ‡¶®‡¶≠‡¶æ‡¶¨‡ßá ‡¶∏‡¶Ç‡¶ú‡ßç‡¶û‡¶æ‡¶Ø‡¶º‡¶ø‡¶§ ‡¶ï‡¶∞‡¶æ ‡¶π‡¶Ø‡¶º‡ßá‡¶õ‡ßá ‡¶Ø‡ßá, ‡¶§‡¶ø‡¶®‡¶ø ‡¶â‡¶¶‡ßç‡¶¶‡ßÄ‡¶™‡¶ï‡ßá‡¶∞ ‡¶∂‡¶æ‡¶´‡¶ø‡¶ï ‡¶ö‡¶∞‡¶ø‡¶§‡ßç‡¶∞‡ßá‡¶∞ ‡¶™‡ßç‡¶∞‡¶§‡¶ø‡¶ß‡ßç‡¶¨‡¶®‡¶ø, ‡¶Ø‡¶ø‡¶®‡¶ø ‡¶â‡¶ö‡ßç‡¶õ‡¶≤, ‡¶∞‡¶∏‡¶ø‡¶ï ‡¶è‡¶¨‡¶Ç ‡¶Ø‡ßá‡¶ï‡ßã‡¶®‡ßã ‡¶™‡¶∞‡¶ø‡¶¨‡ßá‡¶∂‡ßá ‡¶¶‡ßç‡¶∞‡ßÅ‡¶§ ‡¶®‡¶ø‡¶ú‡ßá‡¶ï‡ßá ‡¶Æ‡¶æ‡¶®‡¶ø‡¶Ø‡¶º‡ßá ‡¶®‡¶ø‡¶§‡ßá ‡¶™‡¶æ‡¶∞‡ßá‡¶®‡•§ ‡¶è‡¶õ‡¶æ‡¶°‡¶º‡¶æ, '‡¶Ö‡¶™‡¶∞‡¶ø‡¶ö‡¶ø‡¶§‡¶æ' ‡¶ó‡¶≤‡ßç‡¶™‡ßá ‡¶π‡¶∞‡¶ø‡¶∂‡ßá‡¶∞ ‡¶ó‡ßÅ‡¶£‡ßá‡¶∞ ‡¶Æ‡¶ß‡ßç‡¶Ø‡ßá ‡¶Ü‡¶∏‡¶∞ ‡¶ú‡¶Æ‡¶æ‡¶®‡ßã ‡¶è‡¶¨‡¶Ç ‡¶≠‡¶æ‡¶∑‡¶æ‡¶ü‡¶æ ‡¶Ö‡¶§‡ßç‡¶Ø‡¶®‡ßç‡¶§ ‡¶Ü‡¶Å‡¶ü‡¶∏‡¶æ‡¶Å‡¶ü ‡¶π‡¶ì‡¶Ø‡¶º‡¶æ‡¶∞ ‡¶¨‡¶∞‡ßç‡¶£‡¶®‡¶æ ‡¶™‡¶æ‡¶ì‡¶Ø‡¶º‡¶æ ‡¶Ø‡¶æ‡¶Ø‡¶º‡•§ ‡¶Ö‡¶∞‡ßç‡¶•‡¶æ‡ßé, ‡¶π‡¶∞‡¶ø‡¶∂ ‡¶è‡¶ï‡¶ú‡¶® ‡¶™‡ßç‡¶∞‡¶æ‡¶£‡¶¨‡¶®‡ßç‡¶§, ‡¶∏‡¶æ‡¶Æ‡¶æ‡¶ú‡¶ø‡¶ï‡¶≠‡¶æ‡¶¨‡ßá ‡¶∏‡¶ï‡ßç‡¶∞‡¶ø‡¶Ø‡¶º ‡¶è‡¶¨‡¶Ç ‡¶¨‡ßÅ‡¶¶‡ßç‡¶ß‡¶ø‡¶¶‡ßÄ‡¶™‡ßç‡¶§ ‡¶ö‡¶∞‡¶ø‡¶§‡ßç‡¶∞ ‡¶π‡¶ø‡¶∏‡ßá‡¶¨‡ßá ‡¶ö‡¶ø‡¶§‡ßç‡¶∞‡¶ø‡¶§‡•§
0.802231550216674

























Full result and evaluation matrices are saved as CSV in  rag_eval_results.csv.

üìë API Documentation
Base URL:
http://localhost:8000

POST /ask
Description:
 Submit a question (Bangla or English). The API retrieves relevant textbook passages and uses the RAG model to answer, grounded only in retrieved context.


Request:


Content-Type: application/json


Example:

 json
CopyEdit
{ "question": "‡¶Ö‡¶®‡ßÅ‡¶™‡¶Æ‡ßá‡¶∞ ‡¶≠‡¶æ‡¶∑‡¶æ‡¶Ø‡¶º ‡¶∏‡ßÅ‡¶™‡ßÅ‡¶∞‡ßÅ‡¶∑ ‡¶ï‡¶æ‡¶ï‡ßá ‡¶¨‡¶≤‡¶æ ‡¶π‡¶Ø‡¶º‡ßá‡¶õ‡ßá?" }


Response:


Content-Type: application/json


Example:

 json
CopyEdit
{
  "question": "‡¶Ö‡¶®‡ßÅ‡¶™‡¶Æ‡ßá‡¶∞ ‡¶≠‡¶æ‡¶∑‡¶æ‡¶Ø‡¶º ‡¶∏‡ßÅ‡¶™‡ßÅ‡¶∞‡ßÅ‡¶∑ ‡¶ï‡¶æ‡¶ï‡ßá ‡¶¨‡¶≤‡¶æ ‡¶π‡¶Ø‡¶º‡ßá‡¶õ‡ßá?",
  "answer": "‡¶∂‡ßÅ‡¶Æ‡ßç‡¶≠‡ßÅ‡¶®‡¶æ‡¶•",
  "retrieved_chunks": [
    "[Passage 1] ...",
    "[Passage 2] ...",
    "... up to 5 passages ..."
  ]
}


Behavior:


The model checks passages first, then MCQs/lists/tables.


If the answer is not found, it returns: 'Not found in the book.'



How to run:
pip install fastapi uvicorn pinecone sentence-transformers openai


uvicorn fastapi:app --reload


See /docs for Swagger UI.




üìä Evaluation Matrix
We use two main evaluation strategies:
Answer Similarity (Cosine):


Compute semantic similarity (cosine score using E5) between LLM-generated answer and ground truth.


Results in rag_eval_results.csv.


Retriever Relevance:


For each question, check if the correct answer is present in the top-K retrieved chunks (via string match or semantic similarity).


Results in rag_relevance_eval_results.csv.


 Reflection & Project Questions
1. What method or library did you use to extract the text, and why? Did you face any formatting challenges with the PDF content?
I used easyocr for robust OCR (handles noisy scans, Bangla scripts) and pymupdf for direct text extraction (faster if the PDF has Unicode text).
 Challenges: Bangla textbooks are often scanned images (not Unicode text). This led to garbled output from direct PDF extraction. OCR fixed this but sometimes misrecognized words‚Äîhence, both outputs were cleaned and reconciled with an LLM prompt.

2. What chunking strategy did you choose (e.g., paragraph-based, sentence-based, character limit)? Why do you think it works well for semantic retrieval?
I chunked data by page (per .txt), following the OCR/PDF extraction, then cleaned these into well-formed paragraphs and sections. This keeps context but keeps chunks short enough for LLM and vector search, aiding semantic retrieval.



3. What embedding model did you use? Why did you choose it? How does it capture the meaning of the text?
We used intfloat/multilingual-e5-large from sentence-transformers, a state-of-the-art model for cross-lingual semantic retrieval. It maps both Bangla and English questions/passages to the same embedding space, enabling multilingual RAG.

4. How are you comparing the query with your stored chunks? Why did you choose this similarity method and storage setup?
For retrieval, we use cosine similarity between E5 embeddings of the question and all document chunks, implemented via Pinecone‚Äôs efficient vector search. This allows semantic matching even with different wording.

5. How do you ensure that the question and the document chunks are compared meaningfully? What would happen if the query is vague or missing context?
Both question and chunks are embedded using E5 with appropriate prompts ("query: ...", "passage: ...").


This alignment ensures fair, meaningful comparison.


For vague queries, top-K results may not be directly relevant.
 Potential solution: Increase K, improve chunking, or add fallback strategies.



6. Do the results seem relevant? If not, what might improve them (e.g. better chunking, better embedding model, larger document)?
Most results are highly relevant (cosine similarity near 1 for well-formed queries and answers).
 For borderline cases:
Improving chunking (paragraphs, QAs, MCQs)


Better OCR correction


Larger/more granular chunk set


Using more context windows per question
 can further enhance accuracy and groundedness.




